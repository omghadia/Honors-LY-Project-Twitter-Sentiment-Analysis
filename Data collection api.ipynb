{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install twikit\n",
        "!pip install nltk\n",
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BLTC6lqeosE",
        "outputId": "e7891221-781b-4946-ec84-8786f7ea6a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: twikit in /usr/local/lib/python3.11/dist-packages (2.3.3)\n",
            "Requirement already satisfied: httpx[socks] in /usr/local/lib/python3.11/dist-packages (from twikit) (0.28.1)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from twikit) (1.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from twikit) (4.13.3)\n",
            "Requirement already satisfied: pyotp in /usr/local/lib/python3.11/dist-packages (from twikit) (2.9.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from twikit) (5.3.1)\n",
            "Requirement already satisfied: webvtt-py in /usr/local/lib/python3.11/dist-packages (from twikit) (0.5.1)\n",
            "Requirement already satisfied: m3u8 in /usr/local/lib/python3.11/dist-packages (from twikit) (6.0.0)\n",
            "Requirement already satisfied: Js2Py-3.13 in /usr/local/lib/python3.11/dist-packages (from twikit) (0.74.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->twikit) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->twikit) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx[socks]->twikit) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx[socks]->twikit) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx[socks]->twikit) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx[socks]->twikit) (3.10)\n",
            "Requirement already satisfied: socksio==1.* in /usr/local/lib/python3.11/dist-packages (from httpx[socks]->twikit) (1.0.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx[socks]->twikit) (0.14.0)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.11/dist-packages (from Js2Py-3.13->twikit) (5.3.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from Js2Py-3.13->twikit) (1.17.0)\n",
            "Requirement already satisfied: pyjsparser>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from Js2Py-3.13->twikit) (2.7.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx[socks]->twikit) (1.3.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lemma ke sath"
      ],
      "metadata": {
        "id": "H9BK1Oopcj8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import asyncio\n",
        "from configparser import ConfigParser\n",
        "from datetime import datetime\n",
        "from random import randint\n",
        "from twikit import Client, TooManyRequests\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "MINIMUM_TWEETS = 100\n",
        "QUERIES = ['elon musk', 'artificial intelligence', 'climate change', 'space exploration', 'bitcoin',\n",
        "           'electric vehicles', 'machine learning', 'global economy', 'sports', 'healthcare']\n",
        "\n",
        "\n",
        "async def get_tweets(client, tweets, query):\n",
        "    if tweets is None:\n",
        "        # Get initial tweets\n",
        "        print(f'{datetime.now()} - Getting tweets for \"{query}\"...')\n",
        "        tweets = await client.search_tweet(query, product='Latest')\n",
        "    else:\n",
        "        wait_time = randint(5, 10)\n",
        "        print(f'{datetime.now()} - Getting next tweets for \"{query}\" after {wait_time} seconds...')\n",
        "        await asyncio.sleep(wait_time)\n",
        "        tweets = await tweets.next()\n",
        "\n",
        "    return tweets\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Load login credentials\n",
        "    config = ConfigParser()\n",
        "    config.read('config.ini')\n",
        "    # comment the next 3 lines after cookies.json is created\n",
        "    username = config['X']['username']\n",
        "    email = config['X']['email']\n",
        "    password = config['X']['password']\n",
        "\n",
        "    # Create a CSV file\n",
        "    with open('tweets.csv', 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Query', 'Tweet_count', 'Username', 'Text', 'Created At', 'Retweets', 'Likes', 'Tokenized text', 'Lemmatized text'])\n",
        "\n",
        "    # Authenticate to X.com\n",
        "    client = Client(language='en-US')\n",
        "    # run these next 2 line sof code to first access the site. Once cookies .json is created comment these lines\n",
        "    await client.login(auth_info_1=username, auth_info_2=email, password=password)\n",
        "    client.save_cookies('cookies.json')\n",
        "\n",
        "    # Uncomment out the next line after you have created cookies.json\n",
        "    # client.load_cookies('cookies.json')\n",
        "\n",
        "    for query in QUERIES:\n",
        "        tweet_count = 0\n",
        "        tweets = None\n",
        "\n",
        "        while tweet_count < MINIMUM_TWEETS:\n",
        "            try:\n",
        "                tweets = await get_tweets(client, tweets, query)\n",
        "            except TooManyRequests as e:\n",
        "                rate_limit_reset = datetime.fromtimestamp(e.rate_limit_reset)\n",
        "                print(f'{datetime.now()} - Rate limit reached. Waiting until {rate_limit_reset}')\n",
        "                wait_time = (rate_limit_reset - datetime.now()).total_seconds()\n",
        "                await asyncio.sleep(wait_time)\n",
        "                continue\n",
        "\n",
        "            if not tweets:\n",
        "                print(f'{datetime.now()} - No more tweets found for \"{query}\"')\n",
        "                break\n",
        "\n",
        "            for tweet in tweets:\n",
        "\n",
        "                if tweet.lang != 'en':\n",
        "                  # print(f\"Skipped non-English tweet\")\n",
        "                  continue\n",
        "\n",
        "                tweet_count += 1\n",
        "\n",
        "                # Cleaning text\n",
        "                clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', tweet.text)\n",
        "                clean_text = clean_text.lower()\n",
        "                clean_text = re.sub(r'http\\S+|www\\S+', '', clean_text)\n",
        "\n",
        "                # Tokenize and lemmatize\n",
        "                tokenized_text = word_tokenize(clean_text)\n",
        "                lemmatized_text = [lemmatizer.lemmatize(word) for word in tokenized_text]\n",
        "\n",
        "                tweet_data = [\n",
        "                    query,  # Query name\n",
        "                    tweet_count,  # Sequential tweet count\n",
        "                    getattr(tweet.user, 'name', 'Unknown'),  # Username, default to 'Unknown' if missing\n",
        "                    clean_text if clean_text else '',  # Cleaned text, empty string if missing\n",
        "                    tweet.created_at if tweet.created_at else '',  # Timestamp, empty string if missing\n",
        "                    tweet.retweet_count if tweet.retweet_count else 0,  # Retweet count, default to 0\n",
        "                    tweet.favorite_count if tweet.favorite_count else 0,  # Like count, default to 0\n",
        "                    tokenized_text if tokenized_text else [],  # Tokenized text, default to empty list\n",
        "                    lemmatized_text if lemmatized_text else []  # Lemmatized text, default to empty list\n",
        "                ]\n",
        "\n",
        "                # Append to CSV\n",
        "                with open('tweets.csv', 'a', newline='', encoding='utf-8') as file:\n",
        "                    writer = csv.writer(file)\n",
        "                    writer.writerow(tweet_data)\n",
        "\n",
        "            print(f'{datetime.now()} - Got {tweet_count} tweets for \"{query}\"')\n",
        "\n",
        "        print(f'{datetime.now()} - Done! Got {tweet_count} tweets for \"{query}\"')\n",
        "\n",
        "    print(f'{datetime.now()} - Completed fetching tweets for all queries!')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    await main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYfpcEZlciy4",
        "outputId": "9bfe8134-ae0a-4a25-814a-91d9efa0c341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In order to protect your account from suspicious activity, we've sent a confirmation code to is*********@s******.***. Enter it below to sign in.\t\n",
            ">>> dhk32ixw\n",
            "2025-01-25 13:51:02.027881 - Getting tweets for \"elon musk\"...\n",
            "2025-01-25 13:51:06.583281 - Got 9 tweets for \"elon musk\"\n",
            "2025-01-25 13:51:06.583385 - Getting next tweets for \"elon musk\" after 6 seconds...\n",
            "2025-01-25 13:51:13.032683 - Got 18 tweets for \"elon musk\"\n",
            "2025-01-25 13:51:13.032783 - Getting next tweets for \"elon musk\" after 10 seconds...\n",
            "2025-01-25 13:51:23.582634 - Got 29 tweets for \"elon musk\"\n",
            "2025-01-25 13:51:23.582740 - Getting next tweets for \"elon musk\" after 7 seconds...\n",
            "2025-01-25 13:51:31.333595 - Got 33 tweets for \"elon musk\"\n",
            "2025-01-25 13:51:31.333705 - Getting next tweets for \"elon musk\" after 5 seconds...\n",
            "2025-01-25 13:51:36.753463 - Got 43 tweets for \"elon musk\"\n",
            "2025-01-25 13:51:36.753567 - Getting next tweets for \"elon musk\" after 6 seconds...\n",
            "2025-01-25 13:51:43.227641 - Got 52 tweets for \"elon musk\"\n",
            "2025-01-25 13:51:43.229272 - Getting next tweets for \"elon musk\" after 6 seconds...\n",
            "2025-01-25 13:51:49.680245 - Got 61 tweets for \"elon musk\"\n",
            "2025-01-25 13:51:49.680344 - Getting next tweets for \"elon musk\" after 7 seconds...\n",
            "2025-01-25 13:51:57.170417 - Got 70 tweets for \"elon musk\"\n",
            "2025-01-25 13:51:57.170521 - Getting next tweets for \"elon musk\" after 9 seconds...\n",
            "2025-01-25 13:52:06.554023 - Got 80 tweets for \"elon musk\"\n",
            "2025-01-25 13:52:06.554127 - Getting next tweets for \"elon musk\" after 7 seconds...\n",
            "2025-01-25 13:52:14.406213 - Got 91 tweets for \"elon musk\"\n",
            "2025-01-25 13:52:14.406342 - Getting next tweets for \"elon musk\" after 10 seconds...\n",
            "2025-01-25 13:52:24.833221 - Got 102 tweets for \"elon musk\"\n",
            "2025-01-25 13:52:24.833326 - Done! Got 102 tweets for \"elon musk\"\n",
            "2025-01-25 13:52:24.833484 - Getting tweets for \"artificial intelligence\"...\n",
            "2025-01-25 13:52:25.359152 - Got 15 tweets for \"artificial intelligence\"\n",
            "2025-01-25 13:52:25.359265 - Getting next tweets for \"artificial intelligence\" after 5 seconds...\n",
            "2025-01-25 13:52:30.787632 - Got 28 tweets for \"artificial intelligence\"\n",
            "2025-01-25 13:52:30.787736 - Getting next tweets for \"artificial intelligence\" after 5 seconds...\n",
            "2025-01-25 13:52:36.309650 - Got 40 tweets for \"artificial intelligence\"\n",
            "2025-01-25 13:52:36.309750 - Getting next tweets for \"artificial intelligence\" after 7 seconds...\n",
            "2025-01-25 13:52:43.765740 - Got 53 tweets for \"artificial intelligence\"\n",
            "2025-01-25 13:52:43.765846 - Getting next tweets for \"artificial intelligence\" after 6 seconds...\n",
            "2025-01-25 13:52:50.217682 - Got 68 tweets for \"artificial intelligence\"\n",
            "2025-01-25 13:52:50.217788 - Getting next tweets for \"artificial intelligence\" after 6 seconds...\n",
            "2025-01-25 13:52:56.712915 - Got 85 tweets for \"artificial intelligence\"\n",
            "2025-01-25 13:52:56.713013 - Getting next tweets for \"artificial intelligence\" after 7 seconds...\n",
            "2025-01-25 13:53:04.205354 - Got 97 tweets for \"artificial intelligence\"\n",
            "2025-01-25 13:53:04.205491 - Getting next tweets for \"artificial intelligence\" after 8 seconds...\n",
            "2025-01-25 13:53:12.686070 - Got 113 tweets for \"artificial intelligence\"\n",
            "2025-01-25 13:53:12.686183 - Done! Got 113 tweets for \"artificial intelligence\"\n",
            "2025-01-25 13:53:12.686429 - Getting tweets for \"climate change\"...\n",
            "2025-01-25 13:53:13.095328 - Got 18 tweets for \"climate change\"\n",
            "2025-01-25 13:53:13.095429 - Getting next tweets for \"climate change\" after 8 seconds...\n",
            "2025-01-25 13:53:21.560123 - Got 34 tweets for \"climate change\"\n",
            "2025-01-25 13:53:21.560245 - Getting next tweets for \"climate change\" after 8 seconds...\n",
            "2025-01-25 13:53:30.002922 - Got 51 tweets for \"climate change\"\n",
            "2025-01-25 13:53:30.003024 - Getting next tweets for \"climate change\" after 5 seconds...\n",
            "2025-01-25 13:53:35.414057 - Got 66 tweets for \"climate change\"\n",
            "2025-01-25 13:53:35.414185 - Getting next tweets for \"climate change\" after 6 seconds...\n",
            "2025-01-25 13:53:41.856543 - Got 85 tweets for \"climate change\"\n",
            "2025-01-25 13:53:41.856643 - Getting next tweets for \"climate change\" after 10 seconds...\n",
            "2025-01-25 13:53:52.320012 - Got 102 tweets for \"climate change\"\n",
            "2025-01-25 13:53:52.320106 - Done! Got 102 tweets for \"climate change\"\n",
            "2025-01-25 13:53:52.321511 - Getting tweets for \"space exploration\"...\n",
            "2025-01-25 13:53:52.781766 - Got 16 tweets for \"space exploration\"\n",
            "2025-01-25 13:53:52.781892 - Getting next tweets for \"space exploration\" after 8 seconds...\n",
            "2025-01-25 13:54:01.212341 - Got 35 tweets for \"space exploration\"\n",
            "2025-01-25 13:54:01.212436 - Getting next tweets for \"space exploration\" after 7 seconds...\n",
            "2025-01-25 13:54:08.653801 - Got 53 tweets for \"space exploration\"\n",
            "2025-01-25 13:54:08.653920 - Getting next tweets for \"space exploration\" after 7 seconds...\n",
            "2025-01-25 13:54:16.158728 - Got 68 tweets for \"space exploration\"\n",
            "2025-01-25 13:54:16.158828 - Getting next tweets for \"space exploration\" after 5 seconds...\n",
            "2025-01-25 13:54:21.618260 - Got 83 tweets for \"space exploration\"\n",
            "2025-01-25 13:54:21.618379 - Getting next tweets for \"space exploration\" after 5 seconds...\n",
            "2025-01-25 13:54:27.046050 - Got 96 tweets for \"space exploration\"\n",
            "2025-01-25 13:54:27.046152 - Getting next tweets for \"space exploration\" after 10 seconds...\n",
            "2025-01-25 13:54:37.604268 - Got 111 tweets for \"space exploration\"\n",
            "2025-01-25 13:54:37.605200 - Done! Got 111 tweets for \"space exploration\"\n",
            "2025-01-25 13:54:37.605876 - Getting tweets for \"bitcoin\"...\n",
            "2025-01-25 13:54:37.951670 - Got 8 tweets for \"bitcoin\"\n",
            "2025-01-25 13:54:37.952537 - Getting next tweets for \"bitcoin\" after 5 seconds...\n",
            "2025-01-25 13:54:43.417108 - Got 15 tweets for \"bitcoin\"\n",
            "2025-01-25 13:54:43.417220 - Getting next tweets for \"bitcoin\" after 8 seconds...\n",
            "2025-01-25 13:54:51.856825 - Got 27 tweets for \"bitcoin\"\n",
            "2025-01-25 13:54:51.856922 - Getting next tweets for \"bitcoin\" after 6 seconds...\n",
            "2025-01-25 13:54:58.250158 - Got 37 tweets for \"bitcoin\"\n",
            "2025-01-25 13:54:58.250293 - Getting next tweets for \"bitcoin\" after 8 seconds...\n",
            "2025-01-25 13:55:06.629253 - Got 44 tweets for \"bitcoin\"\n",
            "2025-01-25 13:55:06.629356 - Getting next tweets for \"bitcoin\" after 9 seconds...\n",
            "2025-01-25 13:55:16.047812 - Got 52 tweets for \"bitcoin\"\n",
            "2025-01-25 13:55:16.047910 - Getting next tweets for \"bitcoin\" after 5 seconds...\n",
            "2025-01-25 13:55:21.460715 - Got 61 tweets for \"bitcoin\"\n",
            "2025-01-25 13:55:21.460816 - Getting next tweets for \"bitcoin\" after 5 seconds...\n",
            "2025-01-25 13:55:26.802568 - Got 68 tweets for \"bitcoin\"\n",
            "2025-01-25 13:55:26.802679 - Getting next tweets for \"bitcoin\" after 8 seconds...\n",
            "2025-01-25 13:55:35.168904 - Got 79 tweets for \"bitcoin\"\n",
            "2025-01-25 13:55:35.169002 - Getting next tweets for \"bitcoin\" after 9 seconds...\n",
            "2025-01-25 13:55:44.566385 - Got 88 tweets for \"bitcoin\"\n",
            "2025-01-25 13:55:44.566510 - Getting next tweets for \"bitcoin\" after 8 seconds...\n",
            "2025-01-25 13:55:52.941479 - Got 97 tweets for \"bitcoin\"\n",
            "2025-01-25 13:55:52.941585 - Getting next tweets for \"bitcoin\" after 10 seconds...\n",
            "2025-01-25 13:56:03.306611 - Got 103 tweets for \"bitcoin\"\n",
            "2025-01-25 13:56:03.306702 - Done! Got 103 tweets for \"bitcoin\"\n",
            "2025-01-25 13:56:03.307986 - Getting tweets for \"electric vehicles\"...\n",
            "2025-01-25 13:56:03.749874 - Got 17 tweets for \"electric vehicles\"\n",
            "2025-01-25 13:56:03.749977 - Getting next tweets for \"electric vehicles\" after 7 seconds...\n",
            "2025-01-25 13:56:11.253826 - Got 33 tweets for \"electric vehicles\"\n",
            "2025-01-25 13:56:11.253937 - Getting next tweets for \"electric vehicles\" after 9 seconds...\n",
            "2025-01-25 13:56:20.744723 - Got 51 tweets for \"electric vehicles\"\n",
            "2025-01-25 13:56:20.744820 - Getting next tweets for \"electric vehicles\" after 7 seconds...\n",
            "2025-01-25 13:56:28.223100 - Got 68 tweets for \"electric vehicles\"\n",
            "2025-01-25 13:56:28.223215 - Getting next tweets for \"electric vehicles\" after 10 seconds...\n",
            "2025-01-25 13:56:38.662974 - Got 85 tweets for \"electric vehicles\"\n",
            "2025-01-25 13:56:38.663081 - Getting next tweets for \"electric vehicles\" after 5 seconds...\n",
            "2025-01-25 13:56:44.135774 - Got 103 tweets for \"electric vehicles\"\n",
            "2025-01-25 13:56:44.135863 - Done! Got 103 tweets for \"electric vehicles\"\n",
            "2025-01-25 13:56:44.136091 - Getting tweets for \"machine learning\"...\n",
            "2025-01-25 13:56:44.239669 - Rate limit reached. Waiting until 2025-01-25 14:06:02\n",
            "2025-01-25 14:06:02.048061 - Getting tweets for \"machine learning\"...\n",
            "2025-01-25 14:06:02.605900 - Got 16 tweets for \"machine learning\"\n",
            "2025-01-25 14:06:02.607846 - Getting next tweets for \"machine learning\" after 8 seconds...\n",
            "2025-01-25 14:06:11.003809 - Got 27 tweets for \"machine learning\"\n",
            "2025-01-25 14:06:11.003910 - Getting next tweets for \"machine learning\" after 5 seconds...\n",
            "2025-01-25 14:06:16.422871 - Got 39 tweets for \"machine learning\"\n",
            "2025-01-25 14:06:16.423756 - Getting next tweets for \"machine learning\" after 9 seconds...\n",
            "2025-01-25 14:06:26.014374 - Got 54 tweets for \"machine learning\"\n",
            "2025-01-25 14:06:26.014478 - Getting next tweets for \"machine learning\" after 9 seconds...\n",
            "2025-01-25 14:06:35.513946 - Got 71 tweets for \"machine learning\"\n",
            "2025-01-25 14:06:35.514045 - Getting next tweets for \"machine learning\" after 9 seconds...\n",
            "2025-01-25 14:06:45.003433 - Got 88 tweets for \"machine learning\"\n",
            "2025-01-25 14:06:45.003541 - Getting next tweets for \"machine learning\" after 5 seconds...\n",
            "2025-01-25 14:06:50.404780 - Got 103 tweets for \"machine learning\"\n",
            "2025-01-25 14:06:50.404892 - Done! Got 103 tweets for \"machine learning\"\n",
            "2025-01-25 14:06:50.405085 - Getting tweets for \"global economy\"...\n",
            "2025-01-25 14:06:50.783092 - Got 12 tweets for \"global economy\"\n",
            "2025-01-25 14:06:50.783215 - Getting next tweets for \"global economy\" after 9 seconds...\n",
            "2025-01-25 14:07:00.213302 - Got 28 tweets for \"global economy\"\n",
            "2025-01-25 14:07:00.213398 - Getting next tweets for \"global economy\" after 7 seconds...\n",
            "2025-01-25 14:07:07.695938 - Got 43 tweets for \"global economy\"\n",
            "2025-01-25 14:07:07.696032 - Getting next tweets for \"global economy\" after 7 seconds...\n",
            "2025-01-25 14:07:15.192516 - Got 59 tweets for \"global economy\"\n",
            "2025-01-25 14:07:15.193536 - Getting next tweets for \"global economy\" after 8 seconds...\n",
            "2025-01-25 14:07:23.862696 - Got 77 tweets for \"global economy\"\n",
            "2025-01-25 14:07:23.862795 - Getting next tweets for \"global economy\" after 9 seconds...\n",
            "2025-01-25 14:07:33.262456 - Got 89 tweets for \"global economy\"\n",
            "2025-01-25 14:07:33.262569 - Getting next tweets for \"global economy\" after 5 seconds...\n",
            "2025-01-25 14:07:38.741737 - Got 105 tweets for \"global economy\"\n",
            "2025-01-25 14:07:38.742763 - Done! Got 105 tweets for \"global economy\"\n",
            "2025-01-25 14:07:38.743427 - Getting tweets for \"sports\"...\n",
            "2025-01-25 14:07:39.105057 - Got 11 tweets for \"sports\"\n",
            "2025-01-25 14:07:39.105983 - Getting next tweets for \"sports\" after 7 seconds...\n",
            "2025-01-25 14:07:46.593953 - Got 26 tweets for \"sports\"\n",
            "2025-01-25 14:07:46.594048 - Getting next tweets for \"sports\" after 9 seconds...\n",
            "2025-01-25 14:07:56.209082 - Got 37 tweets for \"sports\"\n",
            "2025-01-25 14:07:56.209241 - Getting next tweets for \"sports\" after 6 seconds...\n",
            "2025-01-25 14:08:02.629780 - Got 47 tweets for \"sports\"\n",
            "2025-01-25 14:08:02.630755 - Getting next tweets for \"sports\" after 6 seconds...\n",
            "2025-01-25 14:08:09.050349 - Got 56 tweets for \"sports\"\n",
            "2025-01-25 14:08:09.050448 - Getting next tweets for \"sports\" after 8 seconds...\n",
            "2025-01-25 14:08:17.447640 - Got 66 tweets for \"sports\"\n",
            "2025-01-25 14:08:17.447731 - Getting next tweets for \"sports\" after 9 seconds...\n",
            "2025-01-25 14:08:26.896322 - Got 80 tweets for \"sports\"\n",
            "2025-01-25 14:08:26.897455 - Getting next tweets for \"sports\" after 10 seconds...\n",
            "2025-01-25 14:08:37.375396 - Got 91 tweets for \"sports\"\n",
            "2025-01-25 14:08:37.375532 - Getting next tweets for \"sports\" after 6 seconds...\n",
            "2025-01-25 14:08:43.853263 - Got 105 tweets for \"sports\"\n",
            "2025-01-25 14:08:43.853358 - Done! Got 105 tweets for \"sports\"\n",
            "2025-01-25 14:08:43.853576 - Getting tweets for \"healthcare\"...\n",
            "2025-01-25 14:08:44.265899 - Got 15 tweets for \"healthcare\"\n",
            "2025-01-25 14:08:44.265999 - Getting next tweets for \"healthcare\" after 9 seconds...\n",
            "2025-01-25 14:08:53.725129 - Got 33 tweets for \"healthcare\"\n",
            "2025-01-25 14:08:53.725249 - Getting next tweets for \"healthcare\" after 7 seconds...\n",
            "2025-01-25 14:09:01.228394 - Got 50 tweets for \"healthcare\"\n",
            "2025-01-25 14:09:01.229409 - Getting next tweets for \"healthcare\" after 8 seconds...\n",
            "2025-01-25 14:09:09.711418 - Got 66 tweets for \"healthcare\"\n",
            "2025-01-25 14:09:09.711526 - Getting next tweets for \"healthcare\" after 7 seconds...\n",
            "2025-01-25 14:09:17.145149 - Got 82 tweets for \"healthcare\"\n",
            "2025-01-25 14:09:17.145270 - Getting next tweets for \"healthcare\" after 8 seconds...\n",
            "2025-01-25 14:09:25.661033 - Got 99 tweets for \"healthcare\"\n",
            "2025-01-25 14:09:25.661135 - Getting next tweets for \"healthcare\" after 8 seconds...\n",
            "2025-01-25 14:09:34.119983 - Got 113 tweets for \"healthcare\"\n",
            "2025-01-25 14:09:34.120093 - Done! Got 113 tweets for \"healthcare\"\n",
            "2025-01-25 14:09:34.121239 - Completed fetching tweets for all queries!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "better lemma hopefully"
      ],
      "metadata": {
        "id": "mQYhAQFSu1yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import asyncio\n",
        "from configparser import ConfigParser\n",
        "from datetime import datetime\n",
        "from random import randint\n",
        "from twikit import Client, TooManyRequests\n",
        "import nltk\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "# Load spaCy's English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "MINIMUM_TWEETS = 90\n",
        "QUERIES = ['elon musk', 'artificial intelligence', 'climate change', 'space exploration', 'bitcoin',\n",
        "           'electric vehicles', 'machine learning', 'global economy', 'sports', 'healthcare']\n",
        "\n",
        "\n",
        "async def get_tweets(client, tweets, query):\n",
        "    if tweets is None:\n",
        "        # Get initial tweets\n",
        "        print(f'{datetime.now()} - Getting tweets for \"{query}\"...')\n",
        "        tweets = await client.search_tweet(query, product='Latest')\n",
        "    else:\n",
        "        wait_time = randint(5, 10)\n",
        "        print(f'{datetime.now()} - Getting next tweets for \"{query}\" after {wait_time} seconds...')\n",
        "        await asyncio.sleep(wait_time)\n",
        "        tweets = await tweets.next()\n",
        "\n",
        "    return tweets\n",
        "\n",
        "def spacy_lemmatize(text):\n",
        "    \"\"\"\n",
        "    Function to lemmatize text using spaCy.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Load login credentials\n",
        "    config = ConfigParser()\n",
        "    config.read('config.ini')\n",
        "    # comment the next 3 lines after cookies.json is created\n",
        "    username = config['X']['username']\n",
        "    email = config['X']['email']\n",
        "    password = config['X']['password']\n",
        "\n",
        "    # Create a CSV file\n",
        "    with open('tweets.csv', 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Query', 'Tweet_count', 'Username', 'Text', 'Created At', 'Retweets', 'Likes', 'Tokenized text', 'Lemmatized text'])\n",
        "\n",
        "    # Authenticate to X.com\n",
        "    client = Client(language='en-US')\n",
        "    # run these next 2 line sof code to first access the site. Once cookies .json is created comment these lines\n",
        "    await client.login(auth_info_1=username, auth_info_2=email, password=password)\n",
        "    client.save_cookies('cookies.json')\n",
        "\n",
        "    # Uncomment out the next line after you have created cookies.json\n",
        "    # client.load_cookies('cookies.json')\n",
        "\n",
        "    for query in QUERIES:\n",
        "        tweet_count = 0\n",
        "        tweets = None\n",
        "\n",
        "        while tweet_count < MINIMUM_TWEETS:\n",
        "            try:\n",
        "                tweets = await get_tweets(client, tweets, query)\n",
        "            except TooManyRequests as e:\n",
        "                rate_limit_reset = datetime.fromtimestamp(e.rate_limit_reset)\n",
        "                print(f'{datetime.now()} - Rate limit reached. Waiting until {rate_limit_reset}')\n",
        "                wait_time = (rate_limit_reset - datetime.now()).total_seconds()\n",
        "                await asyncio.sleep(wait_time)\n",
        "                continue\n",
        "\n",
        "            if not tweets:\n",
        "                print(f'{datetime.now()} - No more tweets found for \"{query}\"')\n",
        "                break\n",
        "\n",
        "            for tweet in tweets:\n",
        "\n",
        "                if tweet.lang != 'en':\n",
        "                  # print(f\"Skipped non-English tweet\")\n",
        "                  continue\n",
        "\n",
        "                tweet_count += 1\n",
        "\n",
        "                # Cleaning text\n",
        "                clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', tweet.text)\n",
        "                clean_text = clean_text.lower()\n",
        "                clean_text = re.sub(r'http\\S+|www\\S+', '', clean_text)\n",
        "\n",
        "                # Tokenize and lemmatize\n",
        "                tokenized_text = word_tokenize(clean_text)\n",
        "                lemmatized_text = spacy_lemmatize(clean_text)\n",
        "\n",
        "                tweet_data = [\n",
        "                    query,  # Query name\n",
        "                    tweet_count,  # Sequential tweet count\n",
        "                    getattr(tweet.user, 'name', 'Unknown'),  # Username, default to 'Unknown' if missing\n",
        "                    clean_text if clean_text else '',  # Cleaned text, empty string if missing\n",
        "                    tweet.created_at if tweet.created_at else '',  # Timestamp, empty string if missing\n",
        "                    tweet.retweet_count if tweet.retweet_count else 0,  # Retweet count, default to 0\n",
        "                    tweet.favorite_count if tweet.favorite_count else 0,  # Like count, default to 0\n",
        "                    tokenized_text if tokenized_text else [],  # Tokenized text, default to empty list\n",
        "                    lemmatized_text if lemmatized_text else []  # Lemmatized text, default to empty list\n",
        "                ]\n",
        "\n",
        "                # Append to CSV\n",
        "                with open('tweets.csv', 'a', newline='', encoding='utf-8') as file:\n",
        "                    writer = csv.writer(file)\n",
        "                    writer.writerow(tweet_data)\n",
        "\n",
        "            print(f'{datetime.now()} - Got {tweet_count} tweets for \"{query}\"')\n",
        "\n",
        "        print(f'{datetime.now()} - Done! Got {tweet_count} tweets for \"{query}\"')\n",
        "\n",
        "    print(f'{datetime.now()} - Completed fetching tweets for all queries!')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    await main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "poc8MV4Auz4r",
        "outputId": "ba3d9d0d-b3ba-403c-b275-a56f3cb1135b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'X'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-41d4203f6198>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mawait\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-41d4203f6198>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'config.ini'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# comment the next 3 lines after cookies.json is created\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0musername\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'username'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0memail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'email'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mpassword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'password'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/configparser.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    977\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_section\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_section\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proxies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'X'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "write how tokenization works\n",
        "difference between nltk lemma vs spacy lemma and why used spacy"
      ],
      "metadata": {
        "id": "vq_mQC246c5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "original"
      ],
      "metadata": {
        "id": "jb61hiAZuRkM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMT5z3URd_ZN",
        "outputId": "2b5385bf-0e19-4f21-c7bf-e0a43beeb379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-01-18 13:22:17.926374 - Getting tweets...\n",
            "2025-01-18 13:22:18.626343 - Got 17 tweets\n",
            "2025-01-18 13:22:18.626442 - Getting next tweets after 6 seconds...\n",
            "2025-01-18 13:22:25.181804 - Got 37 tweets\n",
            "2025-01-18 13:22:25.181925 - Getting next tweets after 10 seconds...\n",
            "2025-01-18 13:22:35.628589 - Got 55 tweets\n",
            "2025-01-18 13:22:35.628694 - Getting next tweets after 7 seconds...\n",
            "2025-01-18 13:22:43.104944 - Got 71 tweets\n",
            "2025-01-18 13:22:43.105052 - Getting next tweets after 8 seconds...\n",
            "2025-01-18 13:22:51.559334 - Got 87 tweets\n",
            "2025-01-18 13:22:51.559450 - Getting next tweets after 10 seconds...\n",
            "2025-01-18 13:23:01.953901 - Got 103 tweets\n",
            "2025-01-18 13:23:01.954002 - Done! Got 103 tweets\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import asyncio\n",
        "from configparser import ConfigParser\n",
        "from datetime import datetime\n",
        "from random import randint\n",
        "from twikit import Client, TooManyRequests\n",
        "import nltk\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# using nitk to tokenize the text for better assessment.\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "MINIMUM_TWEETS = 100\n",
        "QUERY = 'elon musk'  # for whatever word you want to search.\n",
        "\n",
        "\n",
        "async def get_tweets(client, tweets):\n",
        "    if tweets is None:\n",
        "        # Get initial tweets\n",
        "        print(f'{datetime.now()} - Getting tweets...')\n",
        "        tweets = await client.search_tweet(QUERY, product='Latest')\n",
        "    else:\n",
        "        wait_time = randint(5, 10)\n",
        "        print(f'{datetime.now()} - Getting next tweets after {wait_time} seconds...')\n",
        "        await asyncio.sleep(wait_time)\n",
        "        tweets = await tweets.next()\n",
        "\n",
        "    return tweets\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Load login credentials\n",
        "    config = ConfigParser()\n",
        "    config.read('config.ini')\n",
        "    # comment the next 3 lines after cookies.json is created\n",
        "    # username = config['X']['username']\n",
        "    # email = config['X']['email']\n",
        "    # password = config['X']['password']\n",
        "\n",
        "    # Create a CSV file\n",
        "    with open('tweets.csv', 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Tweet_count', 'Username', 'Text', 'Created At', 'Retweets', 'Likes', 'Tokenized text'])\n",
        "\n",
        "    # Authenticate to X.com\n",
        "    client = Client(language='en-US')\n",
        "    # run these next 2 line sof code to first access the site. Once cookies .json is created comment these lines\n",
        "    # await client.login(auth_info_1=username, auth_info_2=email, password=password)\n",
        "    # client.save_cookies('cookies.json')\n",
        "\n",
        "    # Uncomment out the next line after you have created cookies.json\n",
        "    client.load_cookies('cookies.json')\n",
        "\n",
        "    tweet_count = 0\n",
        "    tweets = None\n",
        "\n",
        "    while tweet_count < MINIMUM_TWEETS:\n",
        "        try:\n",
        "            tweets = await get_tweets(client, tweets)\n",
        "        except TooManyRequests as e:\n",
        "            rate_limit_reset = datetime.fromtimestamp(e.rate_limit_reset)\n",
        "            print(f'{datetime.now()} - Rate limit reached. Waiting until {rate_limit_reset}')\n",
        "            wait_time = (rate_limit_reset - datetime.now()).total_seconds()\n",
        "            await asyncio.sleep(wait_time)\n",
        "            continue\n",
        "\n",
        "        if not tweets:\n",
        "            print(f'{datetime.now()} - No more tweets found')\n",
        "            break\n",
        "\n",
        "        for tweet in tweets:\n",
        "            tweet_count += 1\n",
        "            # tweet_data = [tweet_count, tweet.user.name, tweet.text, tweet.created_at, tweet.retweet_count,\n",
        "            #               tweet.favorite_count]\n",
        "            tweet_data = [tweet_count, tweet.user.name, tweet.text, tweet.created_at, tweet.retweet_count, tweet.favorite_count, word_tokenize(tweet.text)]\n",
        "\n",
        "            with open('tweets.csv', 'a', newline='', encoding='utf-8') as file:\n",
        "                writer = csv.writer(file)\n",
        "                writer.writerow(tweet_data)\n",
        "\n",
        "        print(f'{datetime.now()} - Got {tweet_count} tweets')\n",
        "\n",
        "    print(f'{datetime.now()} - Done! Got {tweet_count} tweets')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    await main()\n"
      ]
    }
  ]
}